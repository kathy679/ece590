<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Syllabus | ECE/ENERGY 590</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Analytics -->
  <script>
  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />
</head>

<body>

<div id="header">

  <a href="index.html">
    <h1>ECE/ENERGY 590: Introduction to Machine Learning for Data Science</h1>
  </a>
  <div class='text-center'>
    <h4>Duke University</h4>
    <h4>Spring 2018</h4>
  </div>

  <div style="clear:both;"></div>
</div>


<div class="sechighlight">
<div class="container sec">

  <h2>Schedule and Syllabus</h2>

  <br>
  This course meets Mondays and Wednesdays, from 10:05am - 11:20am in Gross Hall 100C (the Generator)<br>
  <i>Note: ISL = <a href='http://www-bcf.usc.edu/~gareth/ISL/'>Introduction to Statistical Learning</a> (course textbook)</i>

</div>
</div>

<!-- <tr class="warning">    <tr class="danger">    <tr class="info"> -->

<div class="container sec">
<table class="table">
  <tr class="active">
    <th>Event Type</th><th>Date</th><th>Description</th><th>Readings</th><th>Course Materials</th>
  </tr>
  <tr>
    <td>Lecture 1</td>
    <td>Wednesday<br> Jan 10</td>
    <td>
      <b>What is machine learning?</b> <br>
      Course overview and an orientation to the major branches of machine learning: unsupervised, supervised, and reinforcement learning 
    </td>
    <td>None</td>
    <td>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/Lecture01%20-%20What%20is%20machine%20learning.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="info">
    <td>No class</td>
    <td>Monday<br> Jan 15</td>
    <td>Martin Luther King, Jr. Day</td>
    <td></td>
    <td></td>
  </tr>

  <tr class="info">
    <td>Snow Day</td>
    <td>Wednesday<br>Jan 17</td>
    <td>Snow Day</td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Lecture 2</td>
    <td>Monday <br> Jan 22</td>
    <td>
      <b>Core tools I</b> <br>
      Python (numpy, matplotlib, and pandas) and Jupyter notebooks (markdown, code execution, plotting, creating slides, and creating PDFs)
    </td>
    <td>ISL Ch. 1 + 2.1</td>
    <td>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/Lecture02%20-%20Core%20tools%20I.pdf'>[slides]</a>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/python_for_data_science_tutorial.pdf'>[demo slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 3</td>
    <td>Wednesday <br> Jan 24</td>
    <td>
      <b>Core tools II</b> <br>
      Code management best practices and Git version control
    </td>
    <td>ISL Intro of 2.2 and 2.2.1</td>
    <td>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/Lecture03%20-%20Core%20tools%20II.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Monday<br> Jan 29</td>
    <td><b>Assignment #1 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ece590/blob/master/assignments/Assignment%201.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td>Lecture 4</td>
    <td>Monday <br> Jan 29</td>
    <td>
      <b>End-to-end Supervised learning</b> <br>
      We will walk through an example of a supervised machine learning problem from problem formation to performance evaluation and implementation, demonstrating the various components we will explore throughout the semester
    </td>
    <td>ISL 2.2.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/Lecture04%20-%20End-to-end%20Machine%20Learning.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 5</td>
    <td>Wednesday <br> Jan 31</td>
    <td>
      <b>How flexible should my algorithms be: the bias-variance tradeoff </b> <br>
      K-nearest neighbors classification and the bias-variance tradeoff	
      
    </td>
    <td>ISL 2.2.3</td>
    <td>
      <a href='https://github.com/kylebradbury/ece590/raw/master/lectures/Lecture05%20-%20How%20flexible%20should%20my%20algorithms%20be.pdf'>[slides]</a>
    </td>
  </tr>
  
  <tr>
    <td>Lecture 6</td>
    <td>Monday <br> Feb 5</td>
    <td>
      <b>Regression I</b> <br>
      Simple linear regression, multiple linear regression
      
    </td>
    <td>ISL Intro of 3 and 3.1</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 7</td>
    <td>Wednesday <br> Feb 7</td>
    <td>
      <b>Regression II</b> <br>
      Loss functions, measuring error, selecting parameters through least squares and gradient descent
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Monday<br> Feb 12</td>
    <td><b>Assignment #2 Due</b></td>
    <td></td>
    <td><a href='https://github.com/kylebradbury/ece590/blob/master/assignments/Assignment%202.ipynb'>[assignment]</a></td>
  </tr>

  <tr>
    <td>Lecture 8</td>
    <td>Monday <br> Feb 12</td>
    <td>
      <b>Evaluating performance I</b> <br>
      Metrics for classifier performance evaluation: types of errors, receiver operating characteristics curves, confusion matrices
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>  
    <td>Lecture 9</td>
    <td>Wednesday <br> Feb 14</td>
    <td>
      <b>Evaluating performance II</b> <br>
      Resampling techniques: training, testing, and validation datasets, the importance of ensuring representative resampling, and cross validation
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 10</td>
    <td>Monday <br> Feb 19</td>
    <td>
      <b>Selecting models and features: regularization</b> <br>
      Model and feature selection, Occam’s razor. Subset selection, ridge regression, and the lasso
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Feb 21</td>
    <td><b>Assignment #3 Due</b></td>
    <td></td>
    <td>[assignment]</td>
  </tr>

  <tr>
    <td>Lecture 11</td>
    <td>Wednesday <br> Feb 21</td>
    <td>
      <b>Decision theory</b> <br>
      How to operate supervised learning algorithms in practice
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 12</td>
    <td>Monday <br> Feb 26</td>
    <td>
      <b>Dimensionality reduction</b> <br>
      Principal components analysis (PCA) and random projections
    </td>
    <td></td>
    <td>
      [slides]
      <br>
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Feb 28</td>
    <td><b>End of Kaggle Competition</b></td>
    <td></td>
    <td>[kaggle competition]</td>
  </tr>

  <tr>
    <td>Lecture 13</td>
    <td>Wednesday <br> Feb 28</td>
    <td>
      <b>Clustering I</b> <br>
      K-means and hierarchical clustering
   </td>
   <td></td>
   <td>
     [slides]
     <br>
    </td>
  </tr>

  <tr>
    <td>Lecture 14</td>
    <td>Monday <br> Mar 5</td>
    <td>
      <b>Clustering II</b> <br>
      t-SNE, DBSCAN, and Spectral clustering
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Mar 7</td>
    <td><b>Kaggle Competition Reports Due</b></td>
    <td></td>
    <td>[kaggle report]</td>
  </tr>

  <tr>
    <td>Lecture 15</td>
    <td>Wednesday <br> Mar 7</td>
    <td>
      <b>Density estimation</b> <br>
      Kernel density estimation and Gaussian mixture models
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="info">
    <td>No class</td>
    <td>Mar 12-16</td>
    <td>Spring break week</td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Lecture 16</td>
    <td>Monday <br> Mar 19</td>
    <td>
      <b>Other classification methods and a discussion of generative models</b> <br>
      Linear discriminant analysis, Logistic regression, and naïve Bayes
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 17</td>
    <td>Wednesday <br> Mar 21</td>
    <td>
      <b>Ensemble learning</b> <br>
      From decision trees to random forests: bagging, bootstrapping, and boosting
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 18</td>
    <td>Monday <br> Mar 26</td>
    <td>
      <b>Nonlinear transforms and kernel functions</b> <br>
      Nonlinear transformations of predictors and kernel functions
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Mar 28</td>
    <td><b>Assignment #4 Due</b></td>
    <td></td>
    <td>[assignment]</td>
  </tr>

  <tr>
    <td>Lecture 19</td>
    <td>Wednesday <br> Mar 28</td>
    <td>
      <b>Support vector machines</b> <br>
      Support vector machines
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 20</td>
    <td>Monday <br> Apr 2</td>
    <td>
      <b>Neural networks I</b> <br>
      How a neural network works
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 21</td>
    <td>Wednesday <br> Apr 4</td>
    <td>
      <b>Optimization</b> <br>
      Parameter estimation and gradient descent
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 22</td>
    <td>Monday <br> Apr 9</td>
    <td>
      <b>Neural networks II</b> <br>
      Backpropagation and deep learning
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Apr 11</td>
    <td><b>Assignment #5 Due</b></td>
    <td></td>
    <td>[assignment]</td>
  </tr>

  <tr>
    <td>Lecture 23</td>
    <td>Wednesday <br> Apr 11</td>
    <td>
      <b>Overview of supervised learning techniques</b> <br>
      Formulating the reinforcement learning problem
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 24</td>
    <td>Monday <br> Apr 16</td>
    <td>
      <b>The mechanics of reinforcement learning I</b> <br>
      Markov decision processes, Value and policy functions
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 25</td>
    <td>Wednesday <br> Apr 18</td>
    <td>
      <b>The mechanics of reinforcement learning II</b> <br>
      Q-learning and policy search
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 26</td>
    <td>Monday <br> Apr 23</td>
    <td>
      <b>End-to-end reinforcement learning example</b> <br>
      How to put reinforcement learning into practice
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday <br> Apr 25</td>
    <td>
      <b>Final project video showcase and competition</b> <br>
      The grand finale of the semester in which all of the videos you produced will be shown to the class
    </td>
    <td></td>
    <td>
      [final project]
    </td>
  </tr>

</table>
</div>

<div class="sechighlight">
<div id="footer">
  <div id="footermsg">Website design inspired by the <a href="http://cs231n.stanford.edu/">Stanford CS231 course page</a></div>
</div>
</div>

<!-- jQuery and Boostrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
